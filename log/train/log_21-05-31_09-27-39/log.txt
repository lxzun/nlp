description: E128-H768-M24-O128-L12-pretrain
num_epochs: 2000
batch_size: 16
step_batch: 500
eval_batch_size: 256
lr: 0.0002
seed: 42
drop_rate: 0.0
embedding_size: 128
hidden_size: 768
m: 24
out_dim: 128
k: 3
n_layer: 12
max_seq_length: 512
task: pretrain
save_vocab: True
pretrained_vocab_path: 
pretrained_vocab: False
save_model: True
pretrained_model_path: 
pretrained_model: False
use_cuda: cuda
multi_gpu: True
Real used device: cuda

---- dataset info ----

* train data *
- num : 8003752

* eval data *
- num : 10017
----------------------



----------------------- 1 epoch start! -----------------------
epoch:  1/2000	|	batch: 500/500235	|	loss: 4.11901
epoch:  1/2000	|	batch: 1000/500235	|	loss: 1.54097
epoch:  1/2000	|	batch: 1500/500235	|	loss: 1.04699
epoch:  1/2000	|	batch: 2000/500235	|	loss: 0.94207
epoch:  1/2000	|	batch: 2500/500235	|	loss: 0.90661
epoch:  1/2000	|	batch: 3000/500235	|	loss: 0.87082
epoch:  1/2000	|	batch: 3500/500235	|	loss: 0.83620
epoch:  1/2000	|	batch: 4000/500235	|	loss: 0.81158
epoch:  1/2000	|	batch: 4500/500235	|	loss: 0.79411
epoch:  1/2000	|	batch: 5000/500235	|	loss: 0.78372
 >> epoch:  1	|	total_batch: 5000	|	eval_loss: 0.78056896
epoch:  1/2000	|	batch: 5500/500235	|	loss: 0.77160
epoch:  1/2000	|	batch: 6000/500235	|	loss: 0.76506
